{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87433f73-19a9-4526-9ca6-ad7f7ab3eb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader, BinaryDecoder\n",
    "import avro.schema as AvroSchema\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from datetime import datetime, date, timedelta\n",
    "from io import BytesIO\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import max, min, sum, avg, countDistinct, lit, to_timestamp \n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "import pytz\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab8e0a3-a0f9-4c5e-9903-0add38a9f651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = boto3.Session()\n",
    "s3 = session.resource(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "glue_client = session.client('glue')\n",
    "kafka_bucket = s3.Bucket('kafka-messages')\n",
    "\n",
    "url = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\" : \"admin\",\n",
    "    \"password\" : \"admin\"\n",
    "}\n",
    "\n",
    "schema_file = \"schema.avsc\"\n",
    "\n",
    "book_fields = {\n",
    "    'string': ['id', 'name', 'author','category'], \n",
    "    'double': ['quantity', 'price', 'final_price']\n",
    "}\n",
    "\n",
    "book_schema = StructType(\n",
    "    [\n",
    "        StructField(\"report_time\",DateType(),True),\n",
    "        StructField(\"book_id\",StringType(),True),\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"author\",StringType(),True),\n",
    "        StructField(\"category\",StringType(),True),\n",
    "        StructField(\"quantity\",DoubleType(),True),\n",
    "        StructField(\"price\",DoubleType(),True),\n",
    "        StructField(\"final_price\",DoubleType(),True),\n",
    "        StructField(\"created_at\",DateType(),True),\n",
    "        StructField(\"updated_at\",DateType(),True),\n",
    "        StructField(\"deleted_at\",DateType(),True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "book_table_name = 'books'\n",
    "book_key = 'id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b50c5-0abe-497b-b296-dfaeacee172c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Kafka messages reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86fa1ed2-3f2d-4742-891c-41395ec35a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class KafkaMessagesReader:\n",
    "    def __init__(self, bucket, schema_file):\n",
    "        self.bucket = bucket\n",
    "        self.schema = AvroSchema.Parse(open(schema_file, \"r\").read())\n",
    "        self.reader = DatumReader(self.schema)\n",
    "        \n",
    "    def read_avro_message(self, message):\n",
    "        avro_bytes = BytesIO(message)\n",
    "        avro_bytes.seek(5)\n",
    "        decoder = BinaryDecoder(avro_bytes)\n",
    "        return self.reader.read(decoder)\n",
    "    \n",
    "    def get_all_messages(self):\n",
    "        return [message for message in self.bucket.objects.all()]\n",
    "    \n",
    "    def get_new_messages(self):\n",
    "        # Get yesterday messages\n",
    "        today = datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).date()\n",
    "        return [message for message in self.bucket.objects.all() if message.last_modified.replace(tzinfo = None) > datetime(today.year, today.month, today.day - 1, tzinfo = None)]\n",
    "    \n",
    "    def get_new_message_contents(self):\n",
    "        kafka_messages = self.get_new_messages()\n",
    "        return [self.read_avro_message(kafka_message.get()['Body'].read()) for kafka_message in kafka_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff8b37-904b-46ea-a4fb-b7b9bd6911e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Book statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93cece7f-6830-43f8-9857-c906eb403be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BaseActionStatistics:\n",
    "    def __init__(self, fields, messages, key, report_time, data):\n",
    "        self.fields = fields\n",
    "        self.messages = messages\n",
    "        self.key = key\n",
    "        self.report_time = report_time\n",
    "        self.data = data\n",
    "    \n",
    "    def get_field_index(self, field):\n",
    "        return [index for index, value in enumerate(self.fields.get('double')) if value.get('field') == field][0]+len(self.fields.get('string'))+1\n",
    "    \n",
    "    def get_created_message_date(self, message):\n",
    "        return datetime.strptime(message.get('date'), '%a %b %d %Y').date()\n",
    "    \n",
    "    def generate_string_statistics_item_field(self, message):\n",
    "        values = [self.report_time]\n",
    "        values.extend([message.get(field) for field in self.fields.get('string')])\n",
    "        values.extend([float(message.get(field)) for field in self.fields.get('double')])\n",
    "        return values\n",
    "    \n",
    "    def generate_time_statistics_item_field(self, message):\n",
    "        return [None, None, None]\n",
    "    \n",
    "    def generate_statistics_item(self, message):\n",
    "        item = self.generate_string_statistics_item_field(message)\n",
    "        item.extend(self.generate_time_statistics_item_field(message))\n",
    "        return item\n",
    "        \n",
    "    def generate(self):\n",
    "        for message in self.messages:\n",
    "            self.data.update({message.get(self.key):self.generate_statistics_item(message)})\n",
    "        return self.data\n",
    "    \n",
    "class CreateActionStatistics(BaseActionStatistics):\n",
    "    def __init__(self, fields, messages, key, report_time, data):\n",
    "        create_messages = [message for message in messages if message['action'] == 'CREATE']\n",
    "        super().__init__(fields, create_messages, key, report_time, data)\n",
    "    \n",
    "    def generate_time_statistics_item_field(self, message):\n",
    "        return [self.get_created_message_date(message), None, None]\n",
    "    \n",
    "class UpdateActionStatistics(BaseActionStatistics):\n",
    "    def __init__(self, fields, messages, key, report_time, data):\n",
    "        update_messages = [message for message in messages if message['action'] == 'UPDATE']\n",
    "        super().__init__(fields, update_messages, key, report_time, data)\n",
    "        \n",
    "    def generate_time_statistics_item_field(self, message):\n",
    "        created_at = self.data.get(message.get(self.key))[-3] if self.data.get(message.get(self.key)) else None\n",
    "        return [created_at, self.get_created_message_date(message), None]\n",
    "    \n",
    "class DeleteActionStatistics(BaseActionStatistics):\n",
    "    def __init__(self, fields, messages, key, report_time, data):\n",
    "        delete_messages = [message for message in messages if message['action'] == 'DELETE']\n",
    "        super().__init__(fields, delete_messages, key, report_time, data)\n",
    "        \n",
    "    def generate_statistics_item(self, message):\n",
    "        item = self.data.get(message.get(self.key))\n",
    "        item[-1] = self.get_created_message_date(message)\n",
    "        return self.data.get(message.get(self.key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ebc5965-162c-4f7c-b343-f9d7b09269f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BookStatistics:\n",
    "    def __init__(self, fields, schema, table, key, messages, mode, report_time):\n",
    "        self.fields = fields\n",
    "        self.schema = schema\n",
    "        self.table = table\n",
    "        self.key = key\n",
    "        self.messages = messages\n",
    "        self.mode = mode\n",
    "        self.report_time = report_time\n",
    "        self.data = self.get_exist_data(table) if mode == \"append\" else dict()\n",
    "        \n",
    "    def get_latest_data(self, table):\n",
    "        yesterday = datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).date() - timedelta(days=1)\n",
    "        records = spark.read.jdbc(url, table, properties=properties)\n",
    "        return records.filter(records.report_time==yesterday).collect()\n",
    "        \n",
    "    def get_exist_data(self, table):\n",
    "        data = dict()\n",
    "        records = self.get_latest_data(table)\n",
    "        for item in records:\n",
    "            values = [datetime.now(pytz.timezone('Asia/Ho_Chi_Minh'))]\n",
    "            values.extend([value for value in item.asDict().values()][1:])\n",
    "            data.update({item.book_id:values})\n",
    "        return data\n",
    "        \n",
    "    def generate(self):        \n",
    "        self.data = CreateActionStatistics(self.fields, self.messages, self.key, self.report_time, self.data.copy()).generate()\n",
    "        self.data = UpdateActionStatistics(self.fields, self.messages, self.key, self.report_time, self.data.copy()).generate()\n",
    "        self.data = DeleteActionStatistics(self.fields, self.messages, self.key,self.report_time, self.data.copy()).generate()\n",
    "        data = [self.data[item] for item in self.data]\n",
    "        df = spark.createDataFrame(data=data,schema=self.schema)\n",
    "        df.write.jdbc(url=url, table=self.table, mode=self.mode, properties=properties)\n",
    "        df.show(truncate=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65564bd-6a84-4f7f-ad73-e3ae33d72b60",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Category statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "388f8072-eba3-4309-8bc4-25d86476e9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CategoryStatistics:\n",
    "    def __init__(self, book_df, mode):\n",
    "        self.book_df = book_df.filter(\"deleted_at is null\")\n",
    "        self.mode = mode\n",
    "        \n",
    "    def generate(self):\n",
    "        category_df = self.book_df.groupBy('report_time', 'category').agg(\n",
    "            countDistinct('book_id').alias('books'),\n",
    "            countDistinct('author').alias('authors'),\n",
    "            sum('price').alias('price'),\n",
    "            avg('price').alias('average_price'),\n",
    "            sum('final_price').alias('final_price'),\n",
    "            avg('final_price').alias('average_final_price'),\n",
    "            sum('quantity').alias('quantity'),\n",
    "        ).sort('category')\n",
    "        category_df.write.jdbc(url=url, table='category_reports', mode=self.mode, properties=properties)\n",
    "        category_df.show(truncate=False)\n",
    "        return category_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3ad68-9ff5-40e8-b76f-2414d0bcc717",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Author statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9dec0e6-4ec6-4dde-bf18-8bf253669fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AuthorStatistics:\n",
    "    def __init__(self, book_df, mode):\n",
    "        self.book_df = book_df.filter(\"deleted_at is null\")\n",
    "        self.mode = mode\n",
    "        \n",
    "    def generate(self):\n",
    "        author_df = self.book_df.groupBy('report_time', 'author', 'category').agg(\n",
    "            countDistinct('book_id').alias('books'),\n",
    "            sum('price').alias('price'),\n",
    "            avg('price').alias('average_price'),\n",
    "            sum('final_price').alias('final_price'),\n",
    "            avg('final_price').alias('average_final_price'),\n",
    "            sum('quantity').alias('quantity')\n",
    "        ).sort('author')\n",
    "        author_df.write.jdbc(url=url, table='author_reports', mode=self.mode, properties=properties)\n",
    "        author_df.show(truncate=False)\n",
    "        return author_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634d772-9694-47ad-a9a3-751ac8a4fa59",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "239d8dc5-d2b3-4df2-8ed6-bb799ea60207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Report:\n",
    "    def __init__(self, bucket, schema_file, book_fields, book_schema, table=\"book_reports\", book_key=\"id\"):\n",
    "        self.reader = KafkaMessagesReader(bucket, schema_file)\n",
    "        self.book_fields = book_fields\n",
    "        self.book_schema = book_schema        \n",
    "        self.table = table\n",
    "        self.book_key = book_key\n",
    "        self.mode = self.get_mode()\n",
    "        \n",
    "    def get_mode(self):\n",
    "        mode = \"overwrite\" if self.reader.get_new_messages() == self.reader.get_all_messages() else \"append\"\n",
    "        return mode\n",
    "        \n",
    "    def report(self, book_df):\n",
    "        book_df = book_df.filter(\"deleted_at is null\")\n",
    "        report_df = book_df.groupBy('report_time').agg(\n",
    "            countDistinct('book_id').alias('books'),\n",
    "            countDistinct('author').alias('authors'),\n",
    "            countDistinct('category').alias('categories'),\n",
    "            sum('price').alias('price'),\n",
    "            avg('price').alias('average_price'),\n",
    "            sum('final_price').alias('final_price'),\n",
    "            avg('final_price').alias('average_final_price'),\n",
    "            sum('quantity').alias('quantity'),\n",
    "            min('final_price').alias('cheapest'),\n",
    "            max('final_price').alias('most_expensive')\n",
    "        ).sort('report_time')\n",
    "        report_df.write.jdbc(url=url, table='reports', mode=self.mode, properties=properties)\n",
    "        return report_df\n",
    "        \n",
    "    def generate(self):\n",
    "        report_time = datetime.now(pytz.timezone('Asia/Ho_Chi_Minh'))\n",
    "        book_df = BookStatistics(self.book_fields, self.book_schema, self.table, self.book_key, self.reader.get_new_message_contents(), self.mode, report_time).generate()\n",
    "        category_df = CategoryStatistics(book_df, self.mode).generate()\n",
    "        author_df = AuthorStatistics(book_df, self.mode).generate()\n",
    "        return self.report(book_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee7a64fd-3a5f-4819-b498-f8f26ff8e41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+-----------------------------+-----------------------------+--------+--------+-----+-----------+----------+----------+----------+\n",
      "|report_time|book_id                 |name                         |author                       |category|quantity|price|final_price|created_at|updated_at|deleted_at|\n",
      "+-----------+------------------------+-----------------------------+-----------------------------+--------+--------+-----+-----------+----------+----------+----------+\n",
      "|2023-07-27 |64ba0673fa834134be56ffd4|Test new kafka message schema|Test new kafka message schema|Comedy  |100.0   |60.5 |60.5       |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba067ffa834134be56ffd6|Test new kafka message schema|Test new kafka message schema|Sport   |100.0   |100.0|100.0      |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba068bfa834134be56ffd8|Test new kafka message schema|Test new kafka message schema|Sport   |10.0    |79.0 |79.0       |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba069efa834134be56ffda|Test new kafka message schema|Test new kafka message schema|Sport   |56.0    |12.0 |12.0       |2023-07-21|2023-07-21|null      |\n",
      "|2023-07-27 |64ba06b5fa834134be56ffdc|Test new kafka message schema|Test new kafka message schema|Drama   |58.0    |92.0 |92.0       |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba06dafa834134be56ffde|Test new kafka message schema|Test new kafka message schema|Drama   |45.0    |34.0 |34.0       |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba06e1fa834134be56ffe0|Test new kafka message schema|Test new kafka message schema|Comedy  |45.0    |34.0 |34.0       |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba06effa834134be56ffe2|Test new kafka message schema|Test new kafka message schema|Sport   |75.0    |42.0 |42.0       |2023-07-21|2023-07-24|2023-07-26|\n",
      "|2023-07-27 |64ba06f7fa834134be56ffe4|Test new kafka message schema|Test new kafka message schema|Drama   |75.0    |42.0 |42.0       |2023-07-21|null      |2023-07-21|\n",
      "|2023-07-27 |64ba070bfa834134be56ffe6|Test new kafka message schema|Test new kafka message schema|Comedy  |75.0    |70.0 |70.0       |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba0715fa834134be56ffe8|Test new kafka message schema|Test new kafka message schema|Drama   |75.0    |56.0 |56.0       |2023-07-21|2023-07-24|null      |\n",
      "|2023-07-27 |64ba071efa834134be56ffea|Test new kafka message schema|Test new kafka message schema|Sport   |75.0    |70.0 |70.0       |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba0727fa834134be56ffec|Test new kafka message schema|Test new kafka message schema|Drama   |75.0    |70.0 |70.0       |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64ba0795fa834134be570001|Test new kafka message schema|Test new kafka message schema|Sport   |100.0   |100.0|100.0      |2023-07-21|null      |null      |\n",
      "|2023-07-27 |64be255ddee6a319c5b4ba96|Test update name             |Test                         |Drama   |90.0    |90.0 |90.0       |2023-07-24|2023-07-25|2023-07-25|\n",
      "|2023-07-27 |64be2566dee6a319c5b4ba98|Test                         |Test                         |Sport   |90.0    |90.0 |90.0       |2023-07-24|null      |null      |\n",
      "|2023-07-27 |64be256adee6a319c5b4ba9a|Test                         |Test                         |Sport   |90.0    |90.0 |90.0       |2023-07-24|null      |null      |\n",
      "|2023-07-27 |64be2570dee6a319c5b4ba9c|Test                         |Test                         |Sport   |90.0    |90.0 |90.0       |2023-07-24|null      |null      |\n",
      "|2023-07-27 |64bfa0fbad71204626ab8494|Football                     |Author test                  |Sport   |32.0    |57.0 |57.0       |2023-07-25|null      |null      |\n",
      "|2023-07-27 |64bfa115ad71204626ab8496|Tennis                       |Author test 2                |Sport   |36.0    |76.0 |76.0       |2023-07-25|null      |null      |\n",
      "+-----------+------------------------+-----------------------------+-----------------------------+--------+--------+-----+-----------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------+-----+-------+-----+------------------+-----------+-------------------+--------+\n",
      "|report_time|category|books|authors|price|average_price     |final_price|average_final_price|quantity|\n",
      "+-----------+--------+-----+-------+-----+------------------+-----------+-------------------+--------+\n",
      "|2023-07-27 |Comedy  |3    |1      |164.5|54.833333333333336|164.5      |54.833333333333336 |220.0   |\n",
      "|2023-07-27 |Drama   |6    |3      |331.0|55.166666666666664|331.0      |55.166666666666664 |330.0   |\n",
      "|2023-07-27 |Sport   |11   |5      |809.0|73.54545454545455 |809.0      |73.54545454545455  |734.0   |\n",
      "+-----------+--------+-----+-------+-----+------------------+-----------+-------------------+--------+\n",
      "\n",
      "+-----------+-----------------------------+--------+-----+-----+------------------+-----------+-------------------+--------+\n",
      "|report_time|author                       |category|books|price|average_price     |final_price|average_final_price|quantity|\n",
      "+-----------+-----------------------------+--------+-----+-----+------------------+-----------+-------------------+--------+\n",
      "|2023-07-27 |Author test                  |Sport   |1    |57.0 |57.0              |57.0       |57.0               |32.0    |\n",
      "|2023-07-27 |Author test                  |Drama   |1    |34.0 |34.0              |34.0       |34.0               |22.0    |\n",
      "|2023-07-27 |Author test 2                |Sport   |1    |76.0 |76.0              |76.0       |76.0               |36.0    |\n",
      "|2023-07-27 |Test                         |Sport   |3    |270.0|90.0              |270.0      |90.0               |270.0   |\n",
      "|2023-07-27 |Test create book             |Sport   |1    |45.0 |45.0              |45.0       |45.0               |55.0    |\n",
      "|2023-07-27 |Test create book             |Drama   |1    |45.0 |45.0              |45.0       |45.0               |55.0    |\n",
      "|2023-07-27 |Test new kafka message schema|Sport   |5    |361.0|72.2              |361.0      |72.2               |341.0   |\n",
      "|2023-07-27 |Test new kafka message schema|Comedy  |3    |164.5|54.833333333333336|164.5      |54.833333333333336 |220.0   |\n",
      "|2023-07-27 |Test new kafka message schema|Drama   |4    |252.0|63.0              |252.0      |63.0               |253.0   |\n",
      "+-----------+-----------------------------+--------+-----+-----+------------------+-----------+-------------------+--------+\n",
      "\n",
      "root\n",
      " |-- report_time: date (nullable = true)\n",
      " |-- books: long (nullable = false)\n",
      " |-- authors: long (nullable = false)\n",
      " |-- categories: long (nullable = false)\n",
      " |-- price: double (nullable = true)\n",
      " |-- average_price: double (nullable = true)\n",
      " |-- final_price: double (nullable = true)\n",
      " |-- average_final_price: double (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- cheapest: double (nullable = true)\n",
      " |-- most_expensive: double (nullable = true)\n",
      "\n",
      "+-----------+-----+-------+----------+------+-------------+-----------+-------------------+--------+--------+--------------+\n",
      "|report_time|books|authors|categories|price |average_price|final_price|average_final_price|quantity|cheapest|most_expensive|\n",
      "+-----------+-----+-------+----------+------+-------------+-----------+-------------------+--------+--------+--------------+\n",
      "|2023-07-27 |20   |5      |3         |1304.5|65.225       |1304.5     |65.225             |1284.0  |12.0    |100.0         |\n",
      "+-----------+-----+-------+----------+------+-------------+-----------+-------------------+--------+--------+--------------+"
     ]
    }
   ],
   "source": [
    "report_df = Report(kafka_bucket, schema_file, book_fields, book_schema).generate()\n",
    "report_df.printSchema()\n",
    "report_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866bc54-1ead-47a4-84f4-8b71d541ceed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### AWS Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed3a16d1-77a4-4d63-9963-6f12a486f49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class GlueJob:\n",
    "    def __init__(self):\n",
    "        self.context = GlueContext(SparkContext.getOrCreate())\n",
    "        self.job = Job(self.context)\n",
    "        args = self.get_args()\n",
    "        self.job_name = args['JOB_NAME'] if 'JOB_NAME' in args else 'test'\n",
    "        self.job.init(self.job_name, args)\n",
    "    \n",
    "    def get_args(self):\n",
    "        params = ['JOB_NAME'] if '--JOB_NAME' in sys.argv else []\n",
    "        return getResolvedOptions(sys.argv, params)\n",
    "    \n",
    "    def run(self):\n",
    "        if self.job_name == 'report':\n",
    "            report_df = Report(kafka_bucket, schema_file, book_fields, book_schema).generate()\n",
    "            report_df.printSchema()\n",
    "            report_df.show(truncate=False)\n",
    "        self.job.commit()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    GlueJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc0de4-aaa5-44ec-b899-b9e650c34465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
